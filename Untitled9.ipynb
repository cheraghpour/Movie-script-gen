{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FZnz6Xgt4ZkE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "g4oX4Rs54bX3"
   },
   "outputs": [],
   "source": [
    "path_to_file = \"scripts.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2lM65Y94d2_",
    "outputId": "1cb523a9-45d1-414a-bb87-223d3113b4b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 93426 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gAzJl-Vi_L4i",
    "outputId": "785c9a4c-acd2-4855-d7af-9a89e00adeba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM fairest creatures we desire increase,\n",
      "That thereby beauty's rose might never die,\n",
      "But as the riper should by time decease,\n",
      "His tender heir might bear his memory:\n",
      "But thou, contracted to thine own bright eyes,\n",
      "Feed'st thy light'st flame with self\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-90FvAz_RI_",
    "outputId": "307d7e24-8658-42d9-a02d-60b94afd91cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5t5LTdEi_f9o",
    "outputId": "33f0d80f-e0f8-41f4-c95f-af85dc6cc388"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ddaDoabX_ieq"
   },
   "outputs": [],
   "source": [
    "ids_from_chars = preprocessing.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M60p1h2m_mBl",
    "outputId": "9b93bda7-b9e8-42c1-b453-07d1ff8ff324"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[36, 37, 38, 39, 40, 41, 42], [59, 60, 61]]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "aUO2rRC8_n3V"
   },
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o8W1t0sC_q-2",
    "outputId": "419cc4b7-ab6c-48b7-e57e-6a760fe97005"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zDYuXN9B_sur",
    "outputId": "5bfb845d-8fcd-445c-ee0d-8bce539e8b0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sXrrt4Gz_vQE"
   },
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5I2axtg9_xLH",
    "outputId": "5c5667e3-d661-418b-ef91-7df15c678327"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(93426,), dtype=int64, numpy=array([16, 27, 25, ..., 57, 40,  7])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DuBN1uvX_zAH"
   },
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AFtOD9OZ_1Bs",
    "outputId": "9294e225-6ca0-47c1-da61-98d06d9b3fbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "R\n",
      "O\n",
      "M\n",
      " \n",
      "f\n",
      "a\n",
      "i\n",
      "r\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "IwWzaFva_2vY"
   },
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yBBcZTPx_59M",
    "outputId": "1ca13b2a-3eee-4689-a1e1-829bdd9e680e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'R' b'O' b'M' b' ' b'f' b'a' b'i' b'r' b'e' b's' b't' b' ' b'c'\n",
      " b'r' b'e' b'a' b't' b'u' b'r' b'e' b's' b' ' b'w' b'e' b' ' b'd' b'e'\n",
      " b's' b'i' b'r' b'e' b' ' b'i' b'n' b'c' b'r' b'e' b'a' b's' b'e' b','\n",
      " b'\\n' b'T' b'h' b'a' b't' b' ' b't' b'h' b'e' b'r' b'e' b'b' b'y' b' '\n",
      " b'b' b'e' b'a' b'u' b't' b'y' b\"'\" b's' b' ' b'r' b'o' b's' b'e' b' '\n",
      " b'm' b'i' b'g' b'h' b't' b' ' b'n' b'e' b'v' b'e' b'r' b' ' b'd' b'i'\n",
      " b'e' b',' b'\\n' b'B' b'u' b't' b' ' b'a' b's' b' ' b't' b'h' b'e' b' '\n",
      " b'r' b'i' b'p'], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSrLWlyB_791",
    "outputId": "88a58d98-c00f-449a-d372-381f758c88e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"FROM fairest creatures we desire increase,\\nThat thereby beauty's rose might never die,\\nBut as the rip\"\n",
      "b'er should by time decease,\\nHis tender heir might bear his memory:\\nBut thou, contracted to thine own b'\n",
      "b\"right eyes,\\nFeed'st thy light'st flame with self-substantial fuel,\\nMaking a famine where abundance li\"\n",
      "b\"es,\\nThyself thy foe, to thy sweet self too cruel.\\nThou that art now the world's fresh ornament\\nAnd on\"\n",
      "b'ly herald to the gaudy spring,\\nWithin thine own bud buriest thy content\\nAnd, tender churl, makest was'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "M2Zk7qPB_-Zt"
   },
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AzbrSc_CAAiZ",
    "outputId": "ceea3d51-f562-4d65-f90c-6702560fe614"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "f5_BySQIADYQ"
   },
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJKU5Fh9AF5e",
    "outputId": "e6c49b36-8210-433d-abde-20b2139b8717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b\"FROM fairest creatures we desire increase,\\nThat thereby beauty's rose might never die,\\nBut as the ri\"\n",
      "Target: b\"ROM fairest creatures we desire increase,\\nThat thereby beauty's rose might never die,\\nBut as the rip\"\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3P-XdnChAHsw",
    "outputId": "d89d13ad-5a9c-4338-f8d6-14370a509276"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "QdWXYDu_ANFj"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "9eAPj2oIAPvA"
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else: \n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "xbZRkzjYASoV"
   },
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YdBfQk9bAUa1",
    "outputId": "046a755f-717d-4558-ccda-852773062f20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 62) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9TiLf1ByAXaj",
    "outputId": "1f79064d-0068-4a6a-a91a-080121f84fc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  15872     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  63550     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,017,726\n",
      "Trainable params: 4,017,726\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "wKDqlF-eAdxA"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u4Ty0asgAgzf",
    "outputId": "7bed1e20-1043-4893-e6a5-215d7bae2030"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 47, 14, 28,  7, 38, 31, 26, 19, 59,  6, 40, 18, 33, 41, 28, 43,\n",
       "       28,  0,  5,  3, 20, 53, 30, 43, 10, 55, 13, 38, 35, 49,  9, 46, 53,\n",
       "       18, 33, 60,  0,  5, 36, 46, 50, 37, 29, 30, 16, 43, 47, 54, 51,  6,\n",
       "       48, 21, 44, 36, 10, 55, 47, 53,  6,  1,  8, 56, 13, 25, 34, 24, 31,\n",
       "        8,  7, 27, 26, 20, 42, 34, 45, 43, 38,  2, 22, 58, 52,  0, 26, 12,\n",
       "       17, 41, 44, 43, 11, 46, 45, 20, 60, 25, 31, 33, 48, 10, 53])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TBdu7eiEAi3l",
    "outputId": "2b5932be-bcf9-4fd5-b406-5ee119958482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b\"k of day arising\\nFrom sullen earth, sings hymns at heaven's gate;\\nFor thy sweet love remember'd such\"\n",
      "\n",
      "Next Char Predictions:\n",
      " b'HlDS.cVPIx-eHYfShS[UNK],!JrUh?tCc]n;krHYy[UNK],akobTUFhlsp-mKia?tlr-\\n:uCO[NV:.RPJg[jhc Lwq[UNK]PBGfihAkjJyOVYm?r'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "DW9i5TSvAk2j"
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JKVnPSz1AqMw",
    "outputId": "f4b292a0-9cce-49a8-c258-a156223caf32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 62)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         4.128067\n"
     ]
    }
   ],
   "source": [
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pm5d2eosAsGt",
    "outputId": "30d7a3bc-7a6b-4982-c8f3-a3e8f016f4a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.05785"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "9WIuYdAEAu6j"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "mIaYTbl8Aw7X"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './deneme2'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "0h5RaKaXAytt"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 841
    },
    "id": "IK2NRMDlBCWO",
    "outputId": "4967aef0-5ccb-40c8-80ca-7ce3825c1824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "14/14 [==============================] - 65s 4s/step - loss: 4.0648\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 61s 4s/step - loss: 3.3496\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 46s 3s/step - loss: 2.9375\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 43s 3s/step - loss: 2.6864\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 43s 3s/step - loss: 2.4797\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 43s 3s/step - loss: 2.3555\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 43s 3s/step - loss: 2.2855\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 43s 3s/step - loss: 2.2362\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 43s 3s/step - loss: 2.1977\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 43s 3s/step - loss: 2.1612\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 2.1260\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 2.0917\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 41s 3s/step - loss: 2.0552\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 41s 3s/step - loss: 2.0190\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.9821\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.9446\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.9077\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.8735\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 43s 3s/step - loss: 1.8380\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.8072\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.7730\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.7424\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 43s 3s/step - loss: 1.7098\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.6794\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.6520\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.6209\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.5923\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.5633\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.5284\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.4956\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 48s 3s/step - loss: 1.4647\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 41s 3s/step - loss: 1.4297\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 43s 3s/step - loss: 1.3952\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 43s 3s/step - loss: 1.3604\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.3233\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 43s 3s/step - loss: 1.2861\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.2477\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 43s 3s/step - loss: 1.2046\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.1573\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.1131\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.0628\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 43s 3s/step - loss: 1.0091\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.9531\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 47s 3s/step - loss: 0.8980\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.8368\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 0.7748\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 0.7166\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.6536\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 0.5921\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 42s 3s/step - loss: 0.5302\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "lloGkWymBGhl"
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "fVYxatIOD7DO"
   },
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4gvhEDNdD9LF",
    "outputId": "083bc4dc-1dfd-47ef-f87c-7b1dfbaee07e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presting beauty, of thy hos;\n",
      "When thy changr you did impontent;\n",
      "Nor thou wilt doth unvertew thy dear.As it they songur as with Time's waste:\n",
      "Then deach my night death which hath eyes heart's victure than with mine eyes,\n",
      "Within this mecking or such ansence your time,\n",
      "Three beautee therefore to cure, shilior of that wortht strange himbal head.\n",
      "I love to mo,t, all their rank of haling sweet,\n",
      "That''e havour of with that block than lowe-ponsed fair,\n",
      "Or heir it garth the sad on due,\n",
      "Comaling is the stars a fame sumpery spent;\n",
      "Sing the tights to his scoppow upon misplising drais,\n",
      "That you lose to me, now I fas fallest is doob,\n",
      "Love doth waste cliad; do I not grawes hate here's outless my putignts in my state:\n",
      "If it wis me with that put on my love\n",
      "Which should to dway heart's outhts myself behold\n",
      "That host the shadows I one angiving thee:\n",
      "O, others what co my usknord, and my new once ase;\n",
      "And his plessed true morring shows, if his think of tempard.\n",
      "Nothing what is bark compounded is his ompares \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.4209842681884766\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['P'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "\n",
    "print(f\"\\nRun time: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x14a214a90>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
